\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[T1]{fontenc}
\usepackage{multirow}

\author{Sruthy Annie Santhosh}
\author{Diego Coello de Portugal}
\author{Heliya Hasani}
\author{Aditya Nair}
\affil{Supervised by: Mofassir ul Islam Arif}
\title{\textbf{Project Proposal : 
Adversarial Robustness Across Representation Spaces}}
\date{}

\begin{document}

\maketitle

\section{Problem Setting}

When training a deep neural network model to understand an image datasets, a common occurence is that the trained model output changes significantly between an original image and that same image with imperceptible perturbations. This perturbations in the original image to interfere with the model performance are called \textit{Adversarial Attacks}.

One of the fundamental implications of the adversarial attacks is that in some instances where the task corresponds to classification, the model evaluates the image from the adversarial attack as belonging to a class completely different from the correct one, even though the original image is classified correctly. However, the changes done to the original image are so small that the perturbed image also belongs to the same class. This adversarial attacks methods proved that this type of models can be fooled and are somehow inconsistent.
\\

Adversarial learning has become popular due to vulnerability of systems because of not previously pre - defined data into the machine learning algorithm. For instance, self driving car algorithm should not only detect pavements, humans or traffic signs. It should also detect animals or weather for driving security hence, our aim is to create robust environment for these type of datasets which might cause detrimental problems in real life. Adversarial robustness is …. In our project our aim is to create robust environments for different datasets…
\\

Aim of this project is improve adversarial robustness in various datasets using denoising in neural networks . Adversarial robustness is crucial because in real life scenarios there are different type of parameters which are not trained before in current dataset which machine learning algorithm should detect. 

\section{State-of-the-art}

The baseline paper for this project is \textit{Adversarial Robustness across representation spaces} \cite{ARARP}. In this paper the authors mention the importance of training against different attacks methods, since training against specific attacks only improves the robustness against the same type of attacks.
\\

In regards of the main attacks methods used, Fast Gradient Sign Method or FGSM and Project Gradient Method are the most used methods, specially the second one in the most recent papers. Both of them aim to do small pertubations on the image to get an adversarial attack. This perturbations are bounded depending on the representation chosen: pixel based, $L_{p}$ norm and Discrete Cosine Transformed or DCT among others.

As mention previously, to defend against different representation spaces the model also needs to be trained on all of those.
\\

The most frequent way to increase robustness is to train the model with adversarial attacks, where the perturbed images have the same label as the original images since the perturbation is bounded to be small enough to not interfere with the results \cite{EHAE}.
\\

The last attack distinction that exist is whether it knows the model structure and values (\textit{white-box}) or it doesn't (\textit{black-box}). With all this information we can make a small table classifying different attacks, as shown in figure \ref{tab:attacks}.

\begin{table}[h]
    \centering
    \begin{tabular}{|p{1.6cm}||p{2.3cm}|p{1.75cm}|p{1.75cm}|p{2.5cm}|}
        \hline
        Norm & $L_0$ & $L_1$ & $L_2$ & $L_{\infty}$\\
        \hline
        \hline
        White box & SparseFool \cite{Sparsefool}, JSMA \cite{JSMA} & Elastic-net attacks \cite{EAD} & Carlini-Wagner \cite{carlini} & PGD \cite{pgd}, i\_FGSM \cite{ifgsm}, Carlini-Wagner \cite{carlini}\\
        \hline
        Black-box & Adversarial Scratches \cite{scratch}, Sparse-RS \cite{sparse} & - & GenAttack \cite{genattack}, sim \cite{simba} & GenAttack \cite{genattack}, SIMBA \cite{simba}\\
        \hline
    \end{tabular}
    \caption{Attacks classification table}
    \label{tab:attacks}
\end{table}

Even though training against this different attacks increases the robustness against them, it also has been shown that it can affect the performance against the clean dataset, getting a trade off between accuracy and robustness \cite{trade-off}.
\\

Lastly, it is important to mention that this field is a relatively new area of research and there are still some gaps of knowledge in it. For instance, the way of training a model to achieve robustness with the current literature, is closer to a brute force idea rather than designing an intrinsically robust model.

\newpage

\section{Data Foundation}

The datasets that are going to be used in this project can be divided in 2 groups:

\begin{enumerate}
    \item MNIST/FashionMNIST:
    
    These datasets are relatively simple and can be used as a stepping stone in order to test different hipothesis without the added complexity that bigger datasets have.
    
    \item CIFAR-10/ImageNet:
    
    These datasets would be used after an idea has been previously tested with simpler datasets. CIFAR-10 and ImageNet will provide examples closer to real life situations, where there is a high complexity in the datasets that can interfere with the model performance.
    
\end{enumerate}


\section{Research Idea}

We will start by doing an implementation of the attack methods: Fast Gradient Sign Method or FGSM and Project Gradient Method or PGD. In the present, PGD is the main method used for training adversarial robustness models. Nevertheless, FGSM has been shown to have lower computational requirements while holding the same performance.

Both methods will be compared to check their performance against adversarial attacks, but also the trade off in accuracy for the clean dataset.
\\

Once the methods are understood, the project will continue using packages as Adversarial Robustness Toolbox or ART. This package has already the previous attacks implemented and it expands by adding additional features to help training adversarial learning models.
\\

The main results will be tested across different representation spaces, manely on a pixel based, $L_{p}$ norm and DCT. However, this is not enough to prove the robustness and therefore the results will be tested in different datasets to prove its consistency (MNIST, CIFAR-10, ImageNet, etc.).

\newpage

\section{Tangible Outcomes}

We are aiming to publish the work as a research paper.


\section{Work Plan}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Assigment & Timelapse & People Assigned\\
        \hline
        \hline
        Research literature & 01/04 - 30/04 & Whole team\\
        \hline
        Decide solution approaches & 01/05 - 07/05 & Whole team\\
        \hline
        First Implementations & 07/05 - 31/05 & Whole team\\
        \hline
        Prepare first presentation & 01/06 - 07/06 & Whole team\\
        \hline
        \textbf{First presentation} & \textbf{08/06} & \textbf{Whole team}\\
        \hline
        Implement and test different hipothesis & 09/06 - 28/09 & Whole team\\
        \hline
        Prepare second presentation & 29/09 - 05/10 & Whole team\\
        \hline
        \textbf{Second presentation} & \textbf{06/10} & \textbf{Whole team}\\
        \hline
        Final touches and drawing conclusions & 07/10 - 24/11 & Whole team\\
        \hline
        Prepare final presentation & 25/11 - 01/12 & Whole team\\
        \hline
        \textbf{Final presentation} & \textbf{02/12} & \textbf{Whole team}\\
        \hline
        Elaborate final report & 03/12 - 30/03 & Whole team\\
        \hline
        \textbf{Final report} & \textbf{31/03} & \textbf{Whole team}\\
        \hline
    \end{tabular}
    \caption{Timeplan structure}
    \label{tab:my_label}
\end{table}



\section{Team}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Student Name & Student ID & Study course & Semestre\\
        \hline
        \hline
        Santhosh, Sruthy Annie & 312213 & Data Analytics & 2\\
        \hline
        Coello de Portugal, Diego & 312838 & Data Analytics & 2\\
        \hline
        Hasani, Heliya & 311613 & Data Analytics & 3\\
        \hline
        Nair, Aditya & 311014 & Data Analytics & 3\\
        \hline
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}


\newpage
\begin{thebibliography}{9}

\bibitem{ARARP}
Awasthi, Pranjal, et al.
\textit{"Adversarial Robustness Across Representation Spaces."}
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.

\bibitem{Denoise}
Xie, Cihang, et al.
\textit{"Feature denoising for improving adversarial robustness."}
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

\bibitem{Disentangling}
Stutz, David, Matthias Hein, and Bernt Schiele.
\textit{"Disentangling adversarial robustness and generalization."}
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

\bibitem{Local linear}
Qin, Chongli, et al.
\textit{"Adversarial robustness through local linearization."}
arXiv preprint arXiv:1907.02610 (2019).

\bibitem{FGSM}
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi,
Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis,
Gavin Taylor, and Tom Goldstein.
\textit{Adversarial training for
free! In Advances in Neural Information Processing Systems},
pages 3353–3364, 2019.

\bibitem{trade-off}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing,
Laurent El Ghaoui, and Michael I Jordan.
\textit{Theoretically
principled trade-off between robustness and accuracy.}
arXiv preprint arXiv:1901.08573, 2019.

\bibitem{EHAE}
Ian J. Goodfellow, Jonathon Shlens \& Christian Szegedy.
{Explaining and Harnessing Adversarial Examples.}
arXiv:1412.6572v3 [stat.ML] 20 Mar 2015

\bibitem{Sparsefool}
Modas, Apostolos and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal.
\textit{SparseFool: a few pixels make a big difference}.
2018, arXiv:1811.02248 [cs.CV]

\bibitem{JSMA}
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami.
\textit{Practical Black-Box Attacks against Machine Learning.}
2016. arXiv:1602.02697 [cs.CR]

\bibitem{EAD}
Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh.
\textit{EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples}
2017, arXiv:1709.04114v3 [stat.ML].

\bibitem{scratch}
Malhar Jere, Briland Hitaj, Gabriela Ciocarlie, Farinaz Koushanfar.
\textit{Scratch that! An Evolution-based Adversarial Attack against Neural Networks.}
2019,  arXiv:1912.02316v1 [cs.NE].

\bibitem{sparse}
Francesco Croce, Maksym Andriushchenko, Naman D. Singh, Nicolas Flammarion, Matthias Hein
\textit{Sparse-RS: a versatile framework for query-efficient sparse black-box adversarial attacks}.
2020,  arXiv:2006.12834v3 [cs.LG].

\bibitem{carlini}
N. Carlini and D. Wagner.
\textit{Towards Evaluating the Robustness of Neural Networks}.
2017 IEEE Symposium on Security and Privacy (SP), 2017, pp. 39-57, doi: 10.1109/SP.2017.49.

\bibitem{genattack}
Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Mani Srivastava
\textit{GenAttack: Practical Black-box Attacks with Gradient-Free Optimization}.
2018, arXiv:1805.11090v1 [cs.LG].

\bibitem{simba}
Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, Kilian Q. Weinberger.
\textit{Simple Black-box Adversarial Attacks}.
2019, arXiv:1905.07121v2 [cs.LG].

\bibitem{pgd}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu
\textit{Towards Deep Learning Models Resistant to Adversarial Attacks}.
2017, arXiv:1706.06083v4 [stat.ML].

\bibitem{ifgsm}
Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy
\textit{Explaining and Harnessing Adversarial Examples}.
2014, arXiv:1412.6572v3 [stat.ML].



\end{thebibliography}




\end{document}
